{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Spotify Skip Prediction Dataset </h1>\n",
    "This dataset comes in two sets. The first set is details about 'sessions': chunks of songs a user listens to in one go, and what songs were listened to. The second set details the song's features. <br>\n",
    "Our analysis will include just the mini set availible on AI Crowd. The input we are using is an augmented table that combines the user session data and the song features data. \n",
    "There are 167880 entires and 50 total features. Only 47 features will be used. \n",
    "\n",
    "<h3>References</h3>\n",
    "We'd like to recognize that due to enormity of this dataset and the complexity of how it was stored (in multiple seperate and unorganized .csv files), we did use online references to decide on our stack and how we would approach the data. <br>\n",
    "We used the following a examples: <br> <br>\n",
    "<li> <a>https://github.com/a-poor/spotify-skip-prediction/blob/master/README.md</a>\n",
    "<br><i>Used for template tech stack and reorganize dataset. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import Libraries and Datasets</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load both csv files into dataframes\n",
    "log_df = pd.read_csv('log_mini.csv') # user log\n",
    "tf_df  = pd.read_csv('tf_mini.csv')  # track features\n",
    " \n",
    "# rename and merge the two data frames so that the \n",
    "log_df = log_df.rename(columns={'track_id_clean': 'track_id'})\n",
    "\n",
    "# perform a merge so that song information is attached to the user information\n",
    "og_data_df = pd.merge(log_df, tf_df, on='track_id')\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "og_data_df.to_csv('merged_file.csv', index=False)\n",
    "data_df = og_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               session_id  session_position  session_length  \\\n",
      "0  0_00006f66-33e5-4de7-a324-2d18e439fc1e                 1              20   \n",
      "\n",
      "                                 track_id  skip_1  skip_2  skip_3  \\\n",
      "0  t_0479f24c-27d2-46d6-a00c-7ec928f2b539   False   False   False   \n",
      "\n",
      "   not_skipped  context_switch  no_pause_before_play  ...  time_signature  \\\n",
      "0         True               0                     0  ...               4   \n",
      "\n",
      "    valence  acoustic_vector_0  acoustic_vector_1  acoustic_vector_2  \\\n",
      "0  0.152255          -0.815775           0.386409            0.23016   \n",
      "\n",
      "   acoustic_vector_3 acoustic_vector_4  acoustic_vector_5 acoustic_vector_6  \\\n",
      "0           0.028028         -0.333373           0.015452          -0.35359   \n",
      "\n",
      "  acoustic_vector_7  \n",
      "0          0.205826  \n",
      "\n",
      "[1 rows x 50 columns]\n",
      "(167880, 50)\n"
     ]
    }
   ],
   "source": [
    "print(data_df.head(1))\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reorganizing the Data\n",
    "We decided to select the the variable skip_3 as our 'y' variable. The 'skip_3' feautre represents when a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 167880 entries, 0 to 167879\n",
      "Data columns (total 47 columns):\n",
      " #   Column                           Non-Null Count   Dtype  \n",
      "---  ------                           --------------   -----  \n",
      " 0   session_id                       167880 non-null  object \n",
      " 1   session_position                 167880 non-null  int64  \n",
      " 2   session_length                   167880 non-null  int64  \n",
      " 3   track_id                         167880 non-null  object \n",
      " 4   context_switch                   167880 non-null  int64  \n",
      " 5   no_pause_before_play             167880 non-null  int64  \n",
      " 6   short_pause_before_play          167880 non-null  int64  \n",
      " 7   long_pause_before_play           167880 non-null  int64  \n",
      " 8   hist_user_behavior_n_seekfwd     167880 non-null  int64  \n",
      " 9   hist_user_behavior_n_seekback    167880 non-null  int64  \n",
      " 10  hist_user_behavior_is_shuffle    167880 non-null  bool   \n",
      " 11  hour_of_day                      167880 non-null  int64  \n",
      " 12  date                             167880 non-null  object \n",
      " 13  premium                          167880 non-null  bool   \n",
      " 14  context_type                     167880 non-null  object \n",
      " 15  hist_user_behavior_reason_start  167880 non-null  object \n",
      " 16  hist_user_behavior_reason_end    167880 non-null  object \n",
      " 17  duration                         167880 non-null  float64\n",
      " 18  release_year                     167880 non-null  int64  \n",
      " 19  us_popularity_estimate           167880 non-null  float64\n",
      " 20  acousticness                     167880 non-null  float64\n",
      " 21  beat_strength                    167880 non-null  float64\n",
      " 22  bounciness                       167880 non-null  float64\n",
      " 23  danceability                     167880 non-null  float64\n",
      " 24  dyn_range_mean                   167880 non-null  float64\n",
      " 25  energy                           167880 non-null  float64\n",
      " 26  flatness                         167880 non-null  float64\n",
      " 27  instrumentalness                 167880 non-null  float64\n",
      " 28  key                              167880 non-null  int64  \n",
      " 29  liveness                         167880 non-null  float64\n",
      " 30  loudness                         167880 non-null  float64\n",
      " 31  mechanism                        167880 non-null  float64\n",
      " 32  mode                             167880 non-null  object \n",
      " 33  organism                         167880 non-null  float64\n",
      " 34  speechiness                      167880 non-null  float64\n",
      " 35  tempo                            167880 non-null  float64\n",
      " 36  time_signature                   167880 non-null  int64  \n",
      " 37  valence                          167880 non-null  float64\n",
      " 38  acoustic_vector_0                167880 non-null  float64\n",
      " 39  acoustic_vector_1                167880 non-null  float64\n",
      " 40  acoustic_vector_2                167880 non-null  float64\n",
      " 41  acoustic_vector_3                167880 non-null  float64\n",
      " 42  acoustic_vector_4                167880 non-null  float64\n",
      " 43  acoustic_vector_5                167880 non-null  float64\n",
      " 44  acoustic_vector_6                167880 non-null  float64\n",
      " 45  acoustic_vector_7                167880 non-null  float64\n",
      " 46  skipped                          167880 non-null  int32  \n",
      "dtypes: bool(2), float64(25), int32(1), int64(12), object(7)\n",
      "memory usage: 57.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Making a 'skipped' feature for whether a song has been skipped or not, regardless of how fast\n",
    "data_df['skipped'] = (data_df.skip_3 | data_df.skip_2 | data_df.skip_1).astype('int32')\n",
    "\n",
    "# Make 'skipped' column our 'y' value for prediction\n",
    "y_df = data_df['skipped']\n",
    "\n",
    "data_df = data_df.drop(columns=[\"skip_1\", \"skip_2\", \"skip_3\", \"not_skipped\"], axis=1)\n",
    "#Optional print just to check features\n",
    "print(data_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Non-Float Values\n",
    "The IDs of the songs and the users are strings. We've chosen to completely drop these values. While it is reasonable to assume they impact the predicted value, we opt to focus on more generally modeling whether a song will be skipped or not as opposed to whether a song will be skipped or not depending on previous skips and sessions since there are 10,000 sessions in the mini dataset, a value we are not sure how to deal with considering the reasources we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop id values\n",
    "data_df = data_df.drop(columns=[\"session_id\", \"track_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_position  session_length  context_switch  no_pause_before_play  \\\n",
      "0                 1              20               0                     0   \n",
      "1                 7              12               0                     0   \n",
      "2                 6              20               0                     0   \n",
      "\n",
      "   short_pause_before_play  long_pause_before_play  \\\n",
      "0                        0                       0   \n",
      "1                        1                       1   \n",
      "2                        1                       1   \n",
      "\n",
      "   hist_user_behavior_n_seekfwd  hist_user_behavior_n_seekback  \\\n",
      "0                             0                              0   \n",
      "1                             0                              0   \n",
      "2                             0                              0   \n",
      "\n",
      "   hist_user_behavior_is_shuffle  hour_of_day  ...  acoustic_vector_2  \\\n",
      "0                           True           16  ...            0.23016   \n",
      "1                          False           17  ...            0.23016   \n",
      "2                          False           21  ...            0.23016   \n",
      "\n",
      "  acoustic_vector_3 acoustic_vector_4 acoustic_vector_5  acoustic_vector_6  \\\n",
      "0          0.028028         -0.333373          0.015452           -0.35359   \n",
      "1          0.028028         -0.333373          0.015452           -0.35359   \n",
      "2          0.028028         -0.333373          0.015452           -0.35359   \n",
      "\n",
      "   acoustic_vector_7  skipped  session_year  session_month  \\\n",
      "0           0.205826        0          2018              7   \n",
      "1           0.205826        1          2018              7   \n",
      "2           0.205826        0          2018              7   \n",
      "\n",
      "   session_day_of_week  \n",
      "0                    6  \n",
      "1                    6  \n",
      "2                    5  \n",
      "\n",
      "[3 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "# fix the session_date column into seperate parts. dropping day. \n",
    "data_df['session_year'] = pd.to_datetime(data_df['date']).dt.year\n",
    "data_df['session_month'] = pd.to_datetime(data_df['date']).dt.month\n",
    "# data_df['day'] = pd.to_datetime(data_df['date']).dt.day\n",
    "data_df['session_day_of_week'] = pd.to_datetime(data_df['date']).dt.dayofweek\n",
    "#print(data_df.head(3))\n",
    "data_df = data_df.drop('date', axis=1)\n",
    "print(data_df.head(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['premium'] = data_df['premium'].astype(int)\n",
    "# print(data_df['premium'].head(5))\n",
    "\n",
    "# hist_user_behavior_is_shuffle\n",
    "data_df['hist_user_behavior_is_shuffle'] = data_df['hist_user_behavior_is_shuffle'].astype(int)\n",
    "# print(data_df['hist_user_behavior_is_shuffle'].head(5))\n",
    "\n",
    "data_df['mode'] = data_df['mode'].map({'major':1, 'minor':0})\n",
    "# print(data_df['mode'].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables\n",
    "The following variables were categorical in nature:\n",
    "* time_signature\n",
    "* key_signature\n",
    "* context_type\n",
    "* hist_user_behavior_reason_start\t\n",
    "* hist_user_behavior_reason_end\n",
    "<br><br>Lets analyze how many types of values are in each column to determine whether one-hot encoding or ordinal encoding is more advantageous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_signature :  [4 5 3 1 0]\n",
      "key :  [ 1  7 10  8  6  5  4  2  0  3  9 11]\n",
      "context_type :  ['editorial_playlist' 'user_collection' 'catalog' 'radio' 'charts'\n",
      " 'personalized_playlist']\n",
      "hist_user_behavior_reason_start :  ['trackdone' 'fwdbtn' 'appload' 'playbtn' 'clickrow' 'backbtn' 'remote'\n",
      " 'endplay' 'trackerror']\n",
      "hist_user_behavior_reason_end :  ['trackdone' 'endplay' 'fwdbtn' 'backbtn' 'remote' 'logout' 'clickrow']\n"
     ]
    }
   ],
   "source": [
    "list = ['time_signature', 'key', 'context_type', 'hist_user_behavior_reason_start', 'hist_user_behavior_reason_end' ]\n",
    "\n",
    "for col in list:\n",
    "    unique_values = data_df[col].unique()\n",
    "    print(col, \": \", unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Unique Values\n",
    "For *time signature*, due to the ordered nature, we will use *ordinal* encoding.\n",
    "For *context_type*, *key*, *hist_user_behavior_reason_start*, and *hist_user_behavior_reason_end* we will use *one-hot* encoding as their seems to be no inheret order to the values. \n",
    "\n",
    "Let's make the changes now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "categorical_features = ['context_type', 'hist_user_behavior_reason_start', 'hist_user_behavior_reason_end', 'key']\n",
    "encoded_data = encoder.fit_transform(data_df[categorical_features])\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded_data.toarray(), columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "data_df = pd.concat([data_df.reset_index(drop=True), encoded_df], axis=1)\n",
    "\n",
    "data_df.drop(categorical_features, axis=1, inplace=True)\n",
    "\n",
    "# print(data_df.head(2))\n",
    "# print(data_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# check for missing values\n",
    "missing_values = data_df.isnull().sum()\n",
    "\n",
    "# display columns with missing values and counts\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "data_df = data_df.drop(columns=[\"skipped\"], axis=1)\n",
    "# print(data_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Test and Training Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_df, y_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model #1: Logistic Regression\n",
    "Our first model will be a logistic regression model using sklearn's implementation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Parameters: {'logisticregression__C': 0.1, 'logisticregression__penalty': 'l1', 'logisticregression__solver': 'liblinear'}\n",
      "Accuracy: 0.9804920181081725\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97     11769\n",
      "           1       0.98      0.99      0.99     21807\n",
      "\n",
      "    accuracy                           0.98     33576\n",
      "   macro avg       0.98      0.97      0.98     33576\n",
      "weighted avg       0.98      0.98      0.98     33576\n",
      "\n",
      "ROC-AUC Score: 0.988719411471767\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100],  \n",
    "    'logisticregression__penalty': ['l1', 'l2'],  \n",
    "    'logisticregression__solver': ['liblinear']  \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "probabilities = best_model.predict_proba(X_test)[:,1]\n",
    "roc_auc = roc_auc_score(y_test, probabilities)\n",
    "print(\"ROC-AUC Score:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model #2: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  \n",
    "    ('svm', SVC(kernel='linear'))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10],  \n",
    "    'svm__gamma': ['scale', 'auto'], \n",
    "\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "svm_predictions = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "svm_conf_matrix = confusion_matrix(y_test, svm_predictions)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "print(classification_report(y_test, svm_predictions))\n",
    "\n",
    "decision_function = best_model.decision_function(X_test)\n",
    "roc_auc = roc_auc_score(y_test, decision_function)\n",
    "precision, recall, _ = precision_recall_curve(y_test, decision_function)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(\"Confusion Matrix:\\n\", svm_conf_matrix)\n",
    "print(\"ROC-AUC Score:\", roc_auc)\n",
    "print(\"Precision-Recall AUC:\", pr_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model #3: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 19:27:03.034990: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4197/4197 [==============================] - 10s 2ms/step - loss: 0.5710 - accuracy: 0.8962 - val_loss: 0.1344 - val_accuracy: 0.9743\n",
      "Epoch 2/10\n",
      "4197/4197 [==============================] - 9s 2ms/step - loss: 0.1740 - accuracy: 0.9596 - val_loss: 0.1092 - val_accuracy: 0.9785\n",
      "Epoch 3/10\n",
      "4197/4197 [==============================] - 8s 2ms/step - loss: 0.1333 - accuracy: 0.9721 - val_loss: 0.1009 - val_accuracy: 0.9788\n",
      "Epoch 4/10\n",
      "4197/4197 [==============================] - 8s 2ms/step - loss: 0.1176 - accuracy: 0.9747 - val_loss: 0.0996 - val_accuracy: 0.9798\n",
      "Epoch 5/10\n",
      "4197/4197 [==============================] - 8s 2ms/step - loss: 0.1111 - accuracy: 0.9767 - val_loss: 0.0901 - val_accuracy: 0.9784\n",
      "Epoch 6/10\n",
      "4197/4197 [==============================] - 8s 2ms/step - loss: 0.1038 - accuracy: 0.9790 - val_loss: 0.0873 - val_accuracy: 0.9789\n",
      "Epoch 7/10\n",
      "4197/4197 [==============================] - 8s 2ms/step - loss: 0.0992 - accuracy: 0.9792 - val_loss: 0.0929 - val_accuracy: 0.9798\n",
      "Epoch 8/10\n",
      "4197/4197 [==============================] - 8s 2ms/step - loss: 0.0961 - accuracy: 0.9787 - val_loss: 0.0875 - val_accuracy: 0.9795\n",
      "Epoch 9/10\n",
      "4197/4197 [==============================] - 9s 2ms/step - loss: 0.0941 - accuracy: 0.9790 - val_loss: 0.1072 - val_accuracy: 0.9784\n",
      "Epoch 10/10\n",
      "4197/4197 [==============================] - 8s 2ms/step - loss: 0.0955 - accuracy: 0.9785 - val_loss: 0.0901 - val_accuracy: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1537ffa90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050/1050 [==============================] - 1s 1ms/step - loss: 0.0901 - accuracy: 0.9800\n",
      "Test Accuracy: 0.9799857139587402\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
